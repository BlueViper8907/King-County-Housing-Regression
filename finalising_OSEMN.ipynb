{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Housing Regression Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "<font size=3rem>\n",
    "    \n",
    "0 -**[ INTRO](#INTRODUCTION)<br>**\n",
    "1 -**[ OBTAIN](#OBTAIN)**<br>\n",
    "2 -**[ SCRUB](#SCRUB)**<br>\n",
    "3 -**[ EXPLORE](#EXPLORE)**<br>\n",
    "4 -**[ MODEL](#MODEL)**<br>\n",
    "5 -**[ INTERPRET](#INTERPRET)**<br>\n",
    "6 -**[ CONCLUSIONS & RECCOMENDATIONS](#Conclusions-&-Recommendations)<br>**\n",
    "</font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Students: Cody Freese/Fennec Nightingale/Thomas Cornett\n",
    "* Pace: Part time\n",
    "* Instructor: Amber Yandow\n",
    "* Blog post URL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In this notebook we're going to be using the OSEMN model to do OLS regression on housing data from King County in 2015. Here we'll be looking to answer questions like:</p><p> What factors impact the price of a home?</p>\n",
    "<p> What factors impact the price of a home for different income levels?</p>\n",
    "<p> If you're looking to move to king county, where is the best bang for your buck?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from sklearn import svm\n",
    "from scipy.stats import zscore\n",
    "from sklearn import linear_model\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold, train_test_split\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing Our Settings \n",
    "We have too many columns to view normally, and it's difficult to get a good grasp of our data with how much is normally cut off. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('dark_background')\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['lines.color'] = '#FBE122'\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClosest(home_lat: float, home_lon: float, dest_lat_series: 'series', dest_lon_series: 'series'):\n",
    "    \"\"\"Pass 1 set of coordinates and one latitude or longitude column you would like to compare it's distance to\"\"\"\n",
    "    #radius of the earth in miles \n",
    "    r = 3963\n",
    "    #setting variables to use to iterate through  \n",
    "    closest = 100\n",
    "    within_mile = 0\n",
    "    i = 0\n",
    "    #using a while loop to iterate over our data and calculate the distance between each datapoint and our homes \n",
    "    while i < dest_lat_series.size:\n",
    "        lat_dist = radians(home_lat) - (dest_lat := radians(dest_lat_series.iloc[i]))\n",
    "        lon_dist = radians(home_lon) - (radians(dest_lon_series.iloc[i]))\n",
    "        a = sin(lat_dist / 2)**2 + cos(radians(home_lat)) * cos(radians(dest_lat)) * sin(lon_dist / 2)**2\n",
    "        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "        c = r * c \n",
    "        #find the closest data to our homes by keeping our smallest (closest) value\n",
    "        if (c < closest):\n",
    "            closest = c\n",
    "        #find all of the points that fall within one mile and count them \n",
    "        if (c <= 1.0):\n",
    "            within_mile += 1\n",
    "        i += 1\n",
    "    return [closest, within_mile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotcoef(model):\n",
    "    \"\"\"Takes in OLS results and returns a plot of the coefficients\"\"\"\n",
    "    #make dataframe from summary of results \n",
    "    coef_df = pd.DataFrame(model.summary().tables[1].data)\n",
    "    #rename your columns\n",
    "    coef_df.columns = coef_df.iloc[0]\n",
    "    #drop header row \n",
    "    coef_df = coef_df.drop(0)\n",
    "    #set index to variables\n",
    "    coef_df = coef_df.set_index(coef_df.columns[0])\n",
    "    #change dtype from obj to float\n",
    "    coef_df = coef_df.astype(float)\n",
    "    #get errors\n",
    "    err = coef_df['coef'] - coef_df['[0.025']\n",
    "    #append err to end of dataframe \n",
    "    coef_df['errors'] = err\n",
    "    #sort values for plotting \n",
    "    coef_df = coef_df.sort_values(by=['coef'])\n",
    "    ## plotting time ##\n",
    "    var = list(coef_df.index.values)\n",
    "    #add variables column to dataframe \n",
    "    coef_df['var'] = var\n",
    "    # define fig \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    #error bars for 95% confidence interval\n",
    "    coef_df.plot(x='var', y='coef', kind='bar',\n",
    "                ax=ax, fontsize=15, yerr='errors', color='#FBE122', ecolor = '#FBE122')\n",
    "    #set title and label \n",
    "    plt.title('Coefficients of Features With 95% Confidence Interval', fontsize=20)\n",
    "    ax.set_ylabel('Coefficients', fontsize=15)\n",
    "    ax.set_xlabel(' ')\n",
    "    #coefficients \n",
    "    ax.scatter(x= np.arange(coef_df.shape[0]),\n",
    "              marker='+', s=50, \n",
    "              y=coef_df['coef'], color='#FBE122')\n",
    "    plt.legend(fontsize= 15,frameon=True, fancybox=True, facecolor='black')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ols(df, x_columns, target='price'):\n",
    "    \"\"\"Pass in a DataFrame & your predictive columns to return an OLS regression model \"\"\"\n",
    "    #set your x and y variables\n",
    "    X = df[x_columns]\n",
    "    y = df[target]\n",
    "    # pass them into stats models OLS package\n",
    "    ols = sm.OLS(y, X)\n",
    "    #fit your model\n",
    "    model = ols.fit()\n",
    "    #display the model summarry\n",
    "    display(model.summary())\n",
    "    #plot the residuals \n",
    "    fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='r', alpha=.65, fit=True, markerfacecolor=\"y\")\n",
    "    plt.xlim(-2.5, 2.5)\n",
    "    plt.ylim(-2.5, 2.5)\n",
    "    #return model for later use \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN DATA\n",
    "Here we'll be working with the King County housing data provided to us by FlatIron and data about schools in King County gathered by ArcGis. We'll be importing them via the Pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrote up our data types to save on computer space and stop some of them from being inccorectly read as objs\n",
    "kc_dtypes = {'id': int, 'date' : str,  'price': float, 'bedrooms' : int, 'bathrooms' : float, 'sqft_living': int, 'sqft_lot': int, \n",
    "             'floors': float, 'waterfront': float, 'view' : float, 'condition': float, 'grade': int, 'sqft_above': int, \n",
    "             'yr_built': int, 'yr_renovated': float, 'zipcode': float, 'lat': float, 'long': float}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\fenne\\\\Documents\\\\Flatiron\\\\foods.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bfaf89b05c9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkc_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'~\\Documents\\Flatiron\\data\\data\\kc_house_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkc_dtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mschools\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'~\\Documents\\Flatiron\\data\\data\\Schools.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfoods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'~\\Documents\\Flatiron\\foods.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\fenne\\\\Documents\\\\Flatiron\\\\foods.csv'"
     ]
    }
   ],
   "source": [
    "kc_data = pd.read_csv(r'~\\Documents\\Flatiron\\data\\data\\kc_house_data.csv', parse_dates = ['date'], dtype=kc_dtypes)\n",
    "schools = pd.read_csv(r'~\\Documents\\Flatiron\\data\\data\\Schools.csv')\n",
    "foods = pd.read_csv(r'~\\Documents\\Flatiron\\foods.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data = pd.read_csv(r'~\\Documents\\Flatiron\\kc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = foods.loc[foods['lat'] != '[0.0]'].copy()\n",
    "foods = foods.loc[foods['long'] != '[0.0]'].copy()\n",
    "foods['lat'] = foods['lat'].astype(dtype=float)\n",
    "foods['long'] = foods['long'].astype(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = foods.loc[foods['SEAT_CAP'] != 'Grocery']\n",
    "groc = foods.loc[foods['SEAT_CAP'] == 'Grocery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < kc_data['lat'].size:\n",
    "    school = getClosest(kc_data['lat'].iloc[i], kc_data['long'].iloc[i], schools['LAT_CEN'], schools['LONG_CEN'])\n",
    "    restaurant = getClosest(kc_data['lat'].iloc[i], kc_data['long'].iloc[i], rest['lat'], rest['long'])\n",
    "    grocery = getClosest(kc_data['lat'].iloc[i], kc_data['long'].iloc[i], groc['lat'], groc['long'])\n",
    "    kc_dict[i] = {\n",
    "        \"closest school\": school[0],\n",
    "        \"schools within mile\": school[1],\n",
    "        \"closest restaurant\": restaurant[0],\n",
    "        \"restaurants within mile\": restaurant[1],\n",
    "        \"closest grocery\": grocery[0],\n",
    "        \"groceries within mile\": grocery[1]}\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all of the data we need to start. Now we'll be adding the last of our data, merging in the distance between the schools and our homes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc = pd.DataFrame.from_dict(kc_dict, orient='index')\n",
    "kc_data = kc_data.merge(kc, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data = kc_data.rename(columns ={'closest school': 'mi_2_scl', 'schools within mile': 'scls_in_mi', 'closest restaurant':'mi_2_rest', \n",
    "                          'restaurants within mile':'rest_in_mi','closest grocery': 'mi_2_groc', 'groceries within mile': 'groc_in_mi'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our data to see what we are working with and what we might need to fix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB\n",
    "Cleaning up our data, filling NaN values, dropping unnecessary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data = kc_data.drop(['id', 'date'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to use sqft basment later on we need to convert it to a float \n",
    "kc_data['sqft_basement'] = kc_data['sqft_basement'].replace({'?': 0})\n",
    "kc_data['sqft_basement'] = kc_data['sqft_basement'].astype(dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have 3 columns with null values, after exploring them, it makes the most sense to fill the null values with zeros, which is what they had been using to indicate a column without anything anyways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data = kc_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to integer for whole number year, not sure why it'll let us reassign it here but raise errors in dtypes\n",
    "kc_data['yr_renovated'] = kc_data['yr_renovated'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dummy Variables<p>\n",
    "Catagorical columns needs to be transformed so we can use them in our model.</p>\n",
    "<p>\n",
    "thankfully, the Pandas library has got us covered with pd.get_dummies()</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing condition to be a good or bad, hoping that'll help get rid of the multicolinearity \n",
    "kc_data['condition'] = kc_data.condition.replace(to_replace = [1.0, 2.0, 3.0, 4.0, 5.0],  value= ['bad', 'bad', 'good', 'good', 'good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have 70 zipcodes and 120 years, it would add too much complexity to our data to increase it by 190 columns\n",
    "# so instead, we're going to go through and bin them! \n",
    "zips = []\n",
    "years = []\n",
    "\n",
    "\n",
    "for zipcode in kc_data.zipcode:\n",
    "    zips.append(zipcode)\n",
    "for year in kc_data.yr_built:\n",
    "    years.append(year)\n",
    "    \n",
    "zips = list(set(zips))\n",
    "years = list(set(years))\n",
    "\n",
    "zips.sort()\n",
    "years.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will have to find a way to write this into a loop at some point, but, I can't figure out how to get .replace()\n",
    "#to adequatley read lists of lists while also giving them unique names, so for now this works \n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[0:5],  value= 'zip001t005')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[5:10], value= 'zip006t011')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[10:15], value= 'zip014t024')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[15:20], value= 'zip027t031')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[20:25], value= 'zip032t039')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[25:30], value= 'zip040t053')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[30:35], value= 'zip055t065')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[35:40], value= 'zip070t077')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[40:45], value= 'zip092t106')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[45:50], value= 'zip107t115')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[50:55], value= 'zip116t122')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[55:60], value= 'zip125t144')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[60:65], value= 'zip146t168')\n",
    "kc_data['zipcode'] = kc_data.zipcode.replace(to_replace = zips[65:70], value= 'zip177t199')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gonna do the same for year built by 20 years, will give us 6 new columns, may be illuminating \n",
    "kc_data['yr_built'] = kc_data.yr_built.replace(to_replace = years[0:20], value= 'thru20')\n",
    "kc_data['yr_built'] = kc_data.yr_built.replace(to_replace = years[20:40], value= 'thru40')\n",
    "kc_data['yr_built'] = kc_data.yr_built.replace(to_replace = years[40:60], value= 'thru60')\n",
    "kc_data['yr_built'] = kc_data.yr_built.replace(to_replace = years[60:80], value= 'thru80')\n",
    "kc_data['yr_built'] = kc_data.yr_built.replace(to_replace = years[80:100], value= 'thru2000')\n",
    "kc_data['yr_built'] = kc_data.yr_built.replace(to_replace = years[100:120], value= 'thru2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies of our new variables \n",
    "dummys = ['zipcode', 'yr_built', 'condition', ]\n",
    "\n",
    "for dummy in dummys:\n",
    "    dumm = pd.get_dummies(kc_data[dummy], drop_first=True)\n",
    "    kc_data = kc_data.merge(dumm, left_index=True, right_index=True)\n",
    "\n",
    "#we're doing something unique to these variables so it wouldn't save us any time to put them into a loop\n",
    "dumm = pd.get_dummies(kc_data['view'], prefix='view', drop_first=True, dtype=int)\n",
    "kc_data = kc_data.merge(dumm, left_index=True, right_index=True)\n",
    "dumm = pd.get_dummies(kc_data['grade'], prefix='gra', drop_first=True, dtype=int)\n",
    "kc_data = kc_data.merge(dumm, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break up variables into diverse ranges & renaming our dummies so that they'r easier to interpret \n",
    "kc_data = kc_data.rename({'view_1.0': 'view1', 'view_2.0': 'view2', 'view_3.0': 'view3', 'view_4.0':'view4'},axis=1)\n",
    "kc_data = kc_data.rename({'gra_4': 'D', 'gra_5':'Cmin', 'gra_6':'C','gra_7':'Cpl', 'gra_8':'Bmin', 'gra_9':'B',\n",
    "                          'gra_10':'Bpl', 'gra_11':'Amin', 'gra_12':'A', 'gra_13':'Apl'},axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of the data we'll need ready to go we can really start digging in and checking it out! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['scls_in_mi', 'mi_2_scl', 'groc_in_mi', 'mi_2_rest', 'mi_2_groc', 'rest_in_mi'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c35d81213789>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m hist = kc_data[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view',\n\u001b[0m\u001b[0;32m      2\u001b[0m                 \u001b[1;34m'condition'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'grade'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sqft_above'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sqft_basement'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yr_built'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'yr_renovated'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zipcode'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[1;34m'lat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'long'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sqft_living15'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sqft_lot15'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mi_2_scl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'scls_in_mi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mi_2_rest'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 'rest_in_mi', 'mi_2_groc', 'groc_in_mi']]\n\u001b[0;32m      5\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2910\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2912\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1302\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;31m# we skip the warning on Categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['scls_in_mi', 'mi_2_scl', 'groc_in_mi', 'mi_2_rest', 'mi_2_groc', 'rest_in_mi'] not in index\""
     ]
    }
   ],
   "source": [
    "hist = kc_data[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view',\n",
    "                'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', \n",
    "                'lat', 'long', 'sqft_living15', 'sqft_lot15', 'mi_2_scl', 'scls_in_mi', 'mi_2_rest',\n",
    "                'rest_in_mi', 'mi_2_groc', 'groc_in_mi']]\n",
    "hist.hist(figsize=(15,15))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pd.plotting.scatter_matrix(kc_data,figsize=(16,16));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,20))\n",
    "corr = kc_data.corr().abs().round(3)\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap='Oranges', ax=ax)\n",
    "plt.setp(ax.get_xticklabels(), \n",
    "         rotation=45, \n",
    "         ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "ax.set_title('Correlations')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some colinearity between our features, it's best to either remove or transform them if we want to use them in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to multiply by basement to try and get rid of the correlation, we'd be multiplying by a bunch of zeros and it wouldn't adequetly represent our data. By adding one to every 'sqft_basement' that is equal to zero, when we multiply if there are no basement values we still keep our 'sqft_above' values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data['sqft_basement'] = kc_data['sqft_basement'].map(lambda x :  1 if x == 0 else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of multicolinearity in sqftage \n",
    "kc_data['sqft_total'] = kc_data['sqft_living']*kc_data['sqft_lot']\n",
    "kc_data['sqft_neighb'] = kc_data['sqft_living15']*kc_data['sqft_lot15']\n",
    "kc_data['sqft_habitable'] = kc_data['sqft_above']*kc_data['sqft_basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print columns we will be using going forward \n",
    "#make a copy of the dataframe holding only columns we'll be including\n",
    "kc_data.columns\n",
    "all_data = kc_data.copy()\n",
    "kc_data = kc_data[['price', 'bedrooms', 'bathrooms', 'floors','waterfront', \n",
    "                   'yr_renovated', 'lat', 'long', \n",
    "                   'sqft_total', 'sqft_neighb', 'sqft_habitable', \n",
    "                   'good', 'view1', 'view2', 'view3', 'view4', \n",
    "                   'D', 'Cmin', 'C', 'Cpl', 'Bmin', 'B', 'Bpl', 'Amin', \n",
    "                   'zip006t011', 'zip014t024', 'zip027t031', 'zip032t039', \n",
    "                   'zip040t053', 'zip055t065', 'zip070t077', 'zip092t106', \n",
    "                   'zip107t115', 'zip116t122', 'zip125t144', 'zip146t168', \n",
    "                   'zip177t199', \n",
    "                   'thru2000', 'thru2020', 'thru40', 'thru60', 'thru80',\n",
    "                   'mi_2_scl', 'scls_in_mi', 'mi_2_rest', 'rest_in_mi', 'mi_2_groc', 'groc_in_mi']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model on Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowtier = kc_data[kc_data.price <=300000]\n",
    "midtier = kc_data[(kc_data.price > 300001) & (kc_data.price<=800000) ]\n",
    "hightier = kc_data[kc_data.price >800000]\n",
    "\n",
    "lowincome = ['bedrooms', 'bathrooms', 'floors', 'waterfront', \n",
    "          'yr_renovated', 'lat', 'long', \n",
    "          'sqft_total', 'sqft_neighb', 'sqft_habitable', \n",
    "          'good', 'view1', 'view2', 'view3', 'view4', \n",
    "          'D', 'Cmin', 'C', 'Cpl', 'Bmin', 'B', 'Bpl', 'Amin',  \n",
    "          'zip006t011', 'zip014t024', 'zip027t031', 'zip032t039', \n",
    "          'zip040t053', 'zip055t065', 'zip070t077', 'zip092t106', \n",
    "          'zip107t115', 'zip116t122', 'zip125t144', 'zip146t168', \n",
    "          'zip177t199', \n",
    "          'thru2000', 'thru2020', 'thru40', 'thru60', 'thru80',\n",
    "          'mi_2_scl', 'scls_in_mi', 'mi_2_rest', 'rest_in_mi', 'mi_2_groc', 'groc_in_mi']\n",
    "\n",
    "mediumincome = ['bedrooms', 'bathrooms', 'floors', 'waterfront', \n",
    "          'yr_renovated', 'lat', 'long', \n",
    "          'sqft_total', 'sqft_neighb', 'sqft_habitable', \n",
    "          'good', 'view1', 'view2', 'view3', 'view4', \n",
    "          'D', 'Cmin', 'C', 'Cpl', 'Bmin', 'B', 'Bpl', 'Amin',  \n",
    "          'zip006t011', 'zip014t024', 'zip027t031', 'zip032t039', \n",
    "          'zip040t053', 'zip055t065', 'zip070t077', 'zip092t106', \n",
    "          'zip107t115', 'zip116t122', 'zip125t144', 'zip146t168', \n",
    "          'zip177t199', \n",
    "          'thru2000', 'thru2020', 'thru40', 'thru60', 'thru80',\n",
    "          'mi_2_scl', 'scls_in_mi', 'mi_2_rest', 'rest_in_mi', 'mi_2_groc', 'groc_in_mi']\n",
    "\n",
    "highincome = ['bedrooms', 'bathrooms', 'floors', 'waterfront', \n",
    "          'yr_renovated', 'lat', 'long', \n",
    "          'sqft_total', 'sqft_neighb', 'sqft_habitable', \n",
    "          'good', 'view1', 'view2', 'view3', 'view4', \n",
    "          'D', 'Cmin', 'C', 'Cpl', 'Bmin', 'B', 'Bpl', 'Amin',  \n",
    "          'zip006t011', 'zip014t024', 'zip027t031', 'zip032t039', \n",
    "          'zip040t053', 'zip055t065', 'zip070t077', 'zip092t106', \n",
    "          'zip107t115', 'zip116t122', 'zip125t144', 'zip146t168', \n",
    "          'zip177t199', \n",
    "          'thru2000', 'thru2020', 'thru40', 'thru60', 'thru80',\n",
    "          'mi_2_scl', 'scls_in_mi', 'mi_2_rest', 'rest_in_mi', 'mi_2_groc', 'groc_in_mi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_tiers = [('low', lowtier, lowincome), \n",
    "               ('mid', midtier, mediumincome), \n",
    "               ('high', hightier, highincome)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, tier, income in price_tiers:\n",
    "    print(name.upper())\n",
    "    make_ols(tier, income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement\n",
    "First we're going to start filtering out outliers, helping normalize our data should improve our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['price']:\n",
    "    col_zscore = str(col + '_zscore')\n",
    "    kc_data[col_zscore] = (kc_data[col] - kc_data[col].mean())/kc_data[col].std()\n",
    "    kc_data = kc_data.loc[kc_data[col_zscore] < 2]\n",
    "    kc_data = kc_data.loc[kc_data[col_zscore] > (-2)]\n",
    "    kc_data = kc_data.drop(col_zscore, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(kc_data['price'].value_counts().sort_index(), color='#FBE122')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,100):\n",
    "    q = i / 100\n",
    "    print('{} percentile: {}'.format(q, kc_data['price'].quantile(q=q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in bedrooms, we can clearly see a single outlier that is likely just a typo \n",
    "kc_data[kc_data['bedrooms'] == 33]\n",
    "# wouldn't be realistic for a house with 33 bedrooms to only have a sqft_living of 1620 and only 1 3/4 bathrooms so we will adjust to 3 \n",
    "kc_data[kc_data['bedrooms'] == 33] = kc_data[kc_data['bedrooms'] == 33].replace(33,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to fix other outliers we will explore our data and find cutoffs that seem reasonable \n",
    "kc_data = kc_data.loc[kc_data['sqft_total'] <= 1.000000e+09] \n",
    "kc_data = kc_data.loc[kc_data['sqft_total'] >= 400000]\n",
    "kc_data = kc_data.loc[kc_data['sqft_neighb'] <= 1.000000e+09]\n",
    "kc_data = kc_data.loc[kc_data['sqft_habitable'] >= 400000]\n",
    "kc_data = kc_data.loc[kc_data['sqft_habitable'] <= 1.000000e+07]\n",
    "kc_data =  kc_data.loc[kc_data['bathrooms'] >= 1]\n",
    "kc_data =  kc_data.loc[kc_data['bathrooms'] <= 5]\n",
    "kc_data =  kc_data.loc[kc_data['bedrooms'] <= 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowtier = kc_data[(kc_data.price >= 210000) & (kc_data.price <= 348000) ]\n",
    "midtier = kc_data[(kc_data.price >= 348000) & (kc_data.price <= 480000) ]\n",
    "uppermidtier = kc_data[(kc_data.price >= 480000) & (kc_data.price <= 640000) ]\n",
    "hightier = kc_data[(kc_data.price >= 640000) & (kc_data.price <= 900000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowincome = ['bathrooms', 'waterfront', 'lat', 'long',\n",
    "             'sqft_total', 'sqft_habitable', \n",
    "             'view1', 'view2', 'view3', \n",
    "             'C', 'Cpl', 'Bmin', 'B',\n",
    "             'zip040t053', 'zip055t065', 'zip092t106', \n",
    "             'zip107t115', 'zip146t168', \n",
    "             'groc_in_mi']\n",
    "\n",
    "mediumincome = ['bathrooms',  'lat', 'long', \n",
    "                'sqft_habitable', 'view2',   \n",
    "                'Cpl', 'Bmin', 'B', 'Bpl',   \n",
    "                'zip006t011', 'zip014t024', 'zip032t039', \n",
    "                'zip055t065', 'zip070t077', 'zip092t106', \n",
    "                'zip177t199', 'rest_in_mi', 'groc_in_mi',\n",
    "                'thru2000', 'thru2020', 'thru60', 'thru80']\n",
    "\n",
    "uppermedincome = ['bathrooms',  'lat', 'sqft_habitable',   \n",
    "                  'C', 'Bmin', 'B', \n",
    "                  'zip014t024', 'zip027t031', 'zip032t039', \n",
    "                  'zip070t077', 'zip125t144', 'zip146t168', \n",
    "                  'thru2000', 'thru2020', 'thru60', 'thru80']\n",
    "\n",
    "\n",
    "highincome = ['bathrooms', 'floors', 'sqft_neighb', \n",
    "              'sqft_habitable', 'thru2020',\n",
    "              'zip006t011', 'zip107t115',\n",
    "              'zip116t122', 'zip177t199', \n",
    "              'mi_2_scl', 'scls_in_mi', 'mi_2_rest',\n",
    "              'mi_2_groc', 'groc_in_mi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_tiers = [('low', lowtier, lowincome), \n",
    "               ('mid', midtier, mediumincome), \n",
    "               ('upmid', uppermidtier, uppermedincome),\n",
    "               ('high', hightier, highincome)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, tier, income in price_tiers:\n",
    "    print(name.upper())\n",
    "    make_ols(tier, income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Test - High Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step\n",
    "high_data = hightier[['price', 'bathrooms', 'floors', 'sqft_neighb', \n",
    "                      'sqft_habitable', 'thru2020',\n",
    "                      'zip006t011', 'zip107t115',\n",
    "                      'zip116t122', 'zip177t199', \n",
    "                      'mi_2_scl', 'scls_in_mi', 'mi_2_rest',\n",
    "                      'mi_2_groc', 'groc_in_mi']].copy()\n",
    "\n",
    "training_data, testing_data = train_test_split(high_data, test_size=0.25, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns\n",
    "target = 'price'\n",
    "predictive_cols = training_data.drop('price', 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_model = make_ols(hightier, predictive_cols)\n",
    "high_model.params.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_train = high_model.predict(training_data[predictive_cols])\n",
    "y_pred_test = high_model.predict(testing_data[predictive_cols])\n",
    "# then get the scores:\n",
    "train_mse = mean_squared_error(training_data[target], y_pred_train)\n",
    "test_mse = mean_squared_error(testing_data[target], y_pred_test)\n",
    "print('Training MSE:', train_mse, '\\nTesting MSE:', test_mse)\n",
    "print('Training Error: $', sqrt(train_mse), '\\nTesting Error:', sqrt(test_mse))\n",
    "plotcoef(high_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(high_model, \"groc_in_mi\", fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(high_model, \"sqft_habitable\", fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Test - Upper Medium Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step\n",
    "upper_med_data = uppermidtier[['bathrooms',  'lat', 'sqft_habitable',   \n",
    "                               'C', 'Bmin', 'B', 'price',\n",
    "                               'zip014t024', 'zip027t031', 'zip032t039', \n",
    "                               'zip070t077', 'zip125t144', 'zip146t168', \n",
    "                               'thru2000', 'thru2020', 'thru60', 'thru80']].copy()\n",
    "\n",
    "training_data, testing_data = train_test_split(upper_med_data,test_size=0.30, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns\n",
    "target = 'price'\n",
    "predictive_cols = training_data.drop('price', 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uppmid_model = make_ols(training_data, predictive_cols)\n",
    "uppmid_model.params.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_train = uppmid_model.predict(training_data[predictive_cols])\n",
    "y_pred_test = uppmid_model.predict(testing_data[predictive_cols])\n",
    "# then get the scores:\n",
    "train_mse = mean_squared_error(training_data[target], y_pred_train)\n",
    "test_mse = mean_squared_error(testing_data[target], y_pred_test)\n",
    "print('Training MSE:', train_mse, '\\nTesting MSE:', test_mse)\n",
    "print('Training Error: $', sqrt(train_mse), '\\nTesting Error:', sqrt(test_mse))\n",
    "plotcoef(uppmid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(uppmid_model, \"sqft_habitable\", fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Test - Medium Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step\n",
    "mid_data = midtier[['bathrooms',  'lat', 'long', \n",
    "                    'sqft_habitable', 'view2', 'price', \n",
    "                    'Cpl', 'Bmin', 'B', 'Bpl',   \n",
    "                    'zip006t011', 'zip014t024', 'zip032t039', \n",
    "                    'zip055t065', 'zip070t077', 'zip092t106', \n",
    "                    'zip177t199', 'rest_in_mi', 'groc_in_mi',\n",
    "                    'thru2000', 'thru2020', 'thru60', 'thru80']].copy()\n",
    "training_data, testing_data = train_test_split(mid_data, test_size=0.30, random_state=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns\n",
    "target = 'price'\n",
    "predictive_cols = training_data.drop('price', 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_model = make_ols(mid_data, predictive_cols)\n",
    "mid_model.params.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_train = mid_model.predict(training_data[predictive_cols])\n",
    "y_pred_test = mid_model.predict(testing_data[predictive_cols])\n",
    "# then get the scores:\n",
    "train_mse = mean_squared_error(training_data[target], y_pred_train)\n",
    "test_mse = mean_squared_error(testing_data[target], y_pred_test)\n",
    "print('Training MSE:', train_mse, '\\nTesting MSE:', test_mse)\n",
    "print('Training Error: $', sqrt(train_mse), '\\nTesting Error:', sqrt(test_mse))\n",
    "plotcoef(mid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(high_model, \"sqft_habitable\", fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Split Test - Low Income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step\n",
    "low_data = lowtier[['bathrooms', 'waterfront', 'lat', 'long',\n",
    "                    'sqft_total', 'sqft_habitable', \n",
    "                    'view1', 'view2', 'view3', \n",
    "                    'C', 'Cpl', 'Bmin', 'B', 'price',\n",
    "                    'zip040t053', 'zip055t065', 'zip092t106', \n",
    "                    'zip107t115', 'zip146t168', \n",
    "                    'groc_in_mi']].copy()\n",
    "\n",
    "training_data, testing_data = train_test_split(low_data, test_size=0.25, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns\n",
    "target = 'price'\n",
    "predictive_cols = training_data.drop('price', 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_model = make_ols(low_data, predictive_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Low Income Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_model.params.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_train = low_model.predict(training_data[predictive_cols])\n",
    "y_pred_test = low_model.predict(testing_data[predictive_cols])\n",
    "# then get the scores:\n",
    "train_mse = mean_squared_error(training_data[target], y_pred_train)\n",
    "test_mse = mean_squared_error(testing_data[target], y_pred_test)\n",
    "print('Training MSE:', train_mse, '\\nTesting MSE:', test_mse)\n",
    "print('Training Error: $', round(sqrt(train_mse), 2), '\\nTesting Error: $', round(sqrt(test_mse), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcoef(low_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8), color = 'solarized_light')\n",
    "fig = sm.graphics.plot_regress_exog(low_model, \"lat\", fig=fig)\n",
    "plt.show()\n",
    "#as you can see here, the farther west, towards the cities, you go, the more expensive homes become. \n",
    "#please note we are not missing data in those gaps, those represent bodies of water where few people live on small islands or in house boats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(low_model, \"bathrooms\", fig=fig)\n",
    "plt.show()\n",
    "# 6,000$ doesn't look like much compared to prices in the 450,000, but, an increase from 1 to 4 could add over 20k and possibly bump you up to a higher \n",
    "# grade, making your home worth even more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "sns.regplot(x='bathrooms', y='price', data=low_data, ci=95, marker='o', units=.25, color='green', scatter_kws={'s': 95})\n",
    "ax.set(title='Bathooms & Price', \n",
    "       xlabel='Bathrooms', ylabel='Price', alpha =.75)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "- Different incomes have different priorities when it comes to buying or selling a home. In short, Low income tends to put more priority on pragmatic space, While Middle Income homes tend to put more on location and grade, a trend that will increase with importance as you go up income brackets. Here are our best reccomendations. \n",
    "\n",
    "## Reccomendations \n",
    "\n",
    "### *Upper Class*\n",
    "<p>\n",
    "Buyer - If you want to live in Bill Gates neighborhood, near the waterfront, Downtown, or in one of Seattles Art Districts, you'll want to look for home built recently, as homes built after 2000 cost an average of 152,439.24 less. \n",
    "    \n",
    "Seller - Add a loft, it's the most affordable and hip way to increase the number of stories you have and add an average of 115,554.79, while you're at it, we'd reccomend adding another bathroom as well, so long as you don't pass a 1:1 ratio with bedrooms. \n",
    "</p>\n",
    "\n",
    "### *Upper Middle Class* \n",
    "<p>\n",
    "Buyer - Move into a newer, more bland home. Homes built after 1960 cost around 25,000 less than more antique homes, but if they have a slightly above average design, they'll still cost a bit more. However, if you aim for a completely average home without any frills, you're likely to save around an additional 11,274.38, letting you buy an Upper Middle Class home for ~35k less overall. \n",
    "    \n",
    "Seller - Make your homes look nicer by doing things like: adding a nice walkway, garden, some trim or fix up your roof; bringing your grade up to at least a B will increase your house worth by 30,798.13.\n",
    "</p>\n",
    "\n",
    "### *Middle Class* \n",
    "<p>\n",
    "\n",
    "*Buyer* - Tech & buisness heavy neighborhoods like Downtown Seattle/Bellevue, and some of the surrounding areas, dont have very many grocery stores despite being very expensive, living just outside of this range will be cheaper by around a minimum of 11,437.45 and give you more access to grocery stores, while keeping you close enough for a short commute and generally being able to walk everywhere.\n",
    "    \n",
    "*Seller* - Add a balcony, bathroom, and other finishing touches to your home. Giving your home a view of something, and some extra features to increase your grade to a B+ will increase the value by over 56,745.55\n",
    "</p>\n",
    "\n",
    "### *Lower Class*  \n",
    "<p>\n",
    "\n",
    "*Buyer* - To save the most money & have easy access to grocery stores, it's best to live in more rural areas. Moving a few miles farther North or South of downtown Seattle/Bellevue, will save you 10-23k on average, and you can stay away from the city noise/dust while still being in a good distance for adequate public transit or to driving. \n",
    "    \n",
    "*Seller* - In real estate many experts believe the best ratio is a minimum of 2 bathrooms for every 3 bedrooms, and that fitting your property to that ratio will likely increase your value significantly. Our numbers agree with this conclusion, so we reccomend you add a bathroom or two, so long as you don't exceed a 1:1 ratio for bedrooms and bathrooms, each bathroom will add around 6,685.28. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "383.938px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
